---
title: "Bayesian Variational Inference"
author: ''
date: '2021-01-02'
output:
  html_document:
    df_print: paged
categories: []
tags: []
slug: bayesian-variational-inference
---



<div id="bayesian-overview" class="section level1">
<h1>Bayesian Overview</h1>
<p>Bayesian inference can sometimes be very difficult to compute depending on the model settings (assumptions, dimensionality, etc). Heavy computations that often become intractable and some approximation techniques have to be used to address these issues and build fast and scalable systems or algorithms.\
The Bayes rule
<span class="math display">\[
    p(\mathbf{Z} | \mathbf{X}) = \frac{p(\mathbf{X}, \mathbf{Z})}{p(\mathbf{X})} = \frac{p(\mathbf{X} | \mathbf{Z}) p(\mathbf{Z})}{p(\mathbf{X})}
\]</span>
calculates the posterior based on three terms: a prior, a likelihood and an evidence. The first two are easy to get as they are assumed, the third one that is a normalisation factor, requires to be computed such that
<span class="math display">\[
    p(\mathbf{X}) = \int p(\mathbf{X}, \mathbf{Z}) d\mathbf{Z}
\]</span>
In low dimension this integral can be computed, however, it can become intractable in higher dimensions. The exact computation of the posterior distribution requires some approximation techniques. Some other computational difficulties can arise when some variables are discrete. Markov Chain Monte Carlo and Variational Inference methods are the most used to overcome these problems.
# Variational Inference Overview
As discussed above, $ p( | )$ is typically intractable. In variational inference, there are two steps.
## Family of Distribution
The first step is to set up the parametrised family <span class="math inline">\(\mathbf{Q}\)</span> of distributions over the latent variables that defines the space where we search for our best approximation, <span class="math inline">\(q(\mathbf{Z})\)</span>. The complexity of the family determines the complexity of the optimization. The most frequently used form comes from the balance of the bias and the complexity, that is the mean-field variational family as follows
<span class="math display">\[
    q(\mathbf{Z}) = \prod_{m=1}^{M} q_{m}(Z_{m})
\]</span>
where the M-dimensional random latent variables, z, are mutually independent and each governed by a distinct density. \
## Kullback-Leibler divergence
Next, each $q() $ in the family is a candidate approximation. The goal is to find the best candidate <span class="math inline">\(q(\mathbf{Z})\)</span> as approximate distribution close to $ p( | )$, that minimizes the Kullback-Leibler divergence. The following optimization problem should be solved
<span class="math display">\[
q^{*}(Z)  = \underset{q(\mathbf{Z}) \in \mathbf{Q}} {\mathrm{argmin}}\; KL(q(\mathbf{Z})\; || \; p(\mathbf{Z} | \mathbf{X}))
\]</span>
The KL divergence can further be interpreted
<span class="math display">\[
\begin{aligned}
KL(q(\mathbf{Z})\; || \; p(\mathbf{Z} | \mathbf{X}))  &amp; = - \int q(\mathbf{Z}) \ln\frac{p(\mathbf{Z} | \mathbf{X})}{q(\mathbf{Z})} d\mathbf{Z} \\
  &amp; = \int q(\mathbf{Z})\ln q(\mathbf{Z}) d\mathbf{Z} - \int q(\mathbf{Z}) \ln p(\mathbf{Z} | \mathbf{X})d\mathbf{Z} \\
  &amp; = \mathbb{E}\big[ \ln q(\mathbf{Z})\big] - \int q(\mathbf{Z}) \ln \frac{p(\mathbf{X}, \mathbf{Z})}{p(\mathbf{X})} d\mathbf{Z} \\
  &amp; = \mathbb{E}\big[\ln q(\mathbf{Z})\big] + \int q(\mathbf{Z}) \ln p(\mathbf{X})d\mathbf{Z} - \int q(\mathbf{Z}) \ln p(\mathbf{X}, \mathbf{Z})d\mathbf{Z}\\
  &amp; = \mathbb{E}\big[\ln q(\mathbf{Z})\big] + \mathbb{E}\big[\ln p(\mathbf{X})\big] - \mathbb{E}\big[\ln p(\mathbf{X}, \mathbf{Z})\big] \\
  &amp; = \mathbb{E}\big[\ln q(\mathbf{Z})\big]  - \mathbb{E}\big[\ln p(\mathbf{X}, \mathbf{Z})\big] + \ln p(\mathbf{X}) \\
\end{aligned}
\]</span>
## Evidence Lower Bound (ELBO)
The model evidence <span class="math inline">\(\ln p(\mathbf{X})\)</span> is constant and does not depend on the choice of the variational distribution. Instead of solving , we optimize an alternative objective function as follows
<span class="math display">\[
\begin{aligned}
q(Z) &amp; = \mathbb{E}\big[\ln p(\mathbf{X}, \mathbf{Z})\big] - \mathbb{E}\big[\ln q(\mathbf{Z})\big]
\end{aligned}
\]</span>
The function is called evidence lower bound (ELBO). Maximizing ELBO is equivalent to minizing the KL divergence , since the ELBO is the negative KL plus the constant model evidence. ELBO can be rewriten as a sum of the expected log likelihood of the observed data and the KL-divergence between the prior <span class="math inline">\(p(\mathbf{Z})\)</span> and the variational density <span class="math inline">\(q(\mathbf{Z})\)</span>.
<span class="math display">\[
\begin{aligned}
ELBO(q(Z)) &amp; = \mathbb{E}\big[\ln p(\mathbf{X}, \mathbf{Z})\big] - \mathbb{E}\big[\ln q(\mathbf{Z})\big] \\
  &amp; = \mathbb{E}\big[\ln p(\mathbf{X} | \mathbf{Z})\big] + \mathbb{E}\big[\ln p(\mathbf{Z})\big]  - \mathbb{E}\big[\ln q(\mathbf{Z})\big] \\
  &amp; = \mathbb{E}\big[\ln p(\mathbf{X} | \mathbf{Z})\big] - KL(q(\mathbf{Z}) \; ||\; p(\mathbf{Z}))\\
\end{aligned}
\]</span>
## Coordinate Ascent Variational Inference
Combing the ELBO function and the mean-field family, we are then ready for solving the optimization problem. One of the most commonly used algorithms is coordinate ascent variational inference (CAVI) that is introduced by Bishopâ€™s book. CAVI iteratively optimizes each control factor of latent variable <span class="math inline">\(Z_m\)</span> of the mean-field variational density, with the others fixed. It finds the a local optimum of ELBO
<span class="math display">\[
\ln q_{m}^{*}(Z_{m}) = \mathbb{E}\big[\ln p(\mathbf{X}, \mathbf{Z})\big] + \text{constant}
\]</span>
After normalization, the optimal <span class="math inline">\(\{q_m\}\)</span> is
<span class="math display">\[
q_{m}^{*}(Z_{m}) = \frac{\exp\left\{ \mathbb{E}\big[\ln p(\mathbf{X}, \mathbf{Z})\big] \right\} }{\int\exp\left\{ \mathbb{E}\big[\ln p(\mathbf{X}, \mathbf{Z})\big] \right\}d Z_{m}}
\]</span></p>
</div>
